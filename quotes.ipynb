{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Научимся считывать данные. В качестве человеческого текста будем использовать строки из книг, а в качестве кода - код решений для задач по курсу алгоритмам за прошлый семестр.\n",
    "Всего строк текста около 8к, кода даже больше, чем нужно, поэтому возьмем, чтобы его было 20%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text(path: str, mode='book'):\n",
    "    \"\"\"\n",
    "    Для чтения возможно 2 случая - для книг и для кода\n",
    "    В случае чтения файла с кодом, дополнительно убираются все комментарии и отступы в начале строки\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    with open(path, 'r') as text:\n",
    "        flag = True\n",
    "        for s in text:\n",
    "            line = ''\n",
    "            if s!='\\n':\n",
    "                line = s.lower().replace('\\n', '').replace('\\t', '')\n",
    "            if mode=='code':\n",
    "                line = ' '.join(map(str, line.split()))\n",
    "                if '#' in line:\n",
    "                    line = line[:line.find('#')]\n",
    "                if \"'''\" in line:\n",
    "                    flag = not flag\n",
    "            if flag and \"'''\" not in line and line != '':\n",
    "                X.append(line)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join, isdir\n",
    "\n",
    "def read_book(mypath: str):\n",
    "    \"\"\"\n",
    "    Обход .txt файлов\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for f in listdir(mypath):\n",
    "        X.extend(read_text(join(mypath, f)))\n",
    "    return X\n",
    "\n",
    "def read_code(mypath: str):\n",
    "    \"\"\"\n",
    "    Обход .py файлов в папочках\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    # for d in listdir(mypath):\n",
    "    #     new_path = join(mypath, d)\n",
    "    #     if isdir(new_path):\n",
    "    #         for f in listdir(new_path):\n",
    "    #             X.extend(read_text(join(new_path, f), mode='code'))\n",
    "    for f in listdir(mypath):\n",
    "        X.extend(read_text(join(mypath, f), mode='code'))\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def read_dataset(book_path: str, code_path: str):\n",
    "    \"\"\"\n",
    "    Собираем датасет\n",
    "    0 - человеческий текст, 1 - код\n",
    "    \"\"\"\n",
    "    X1 = read_book(book_path)\n",
    "    X2 = read_code(code_path)[:int(len(X1) * 0.16)]\n",
    "    # print(X2[:100])\n",
    "    print(f'books {len(X1)}\\ncode {len(X2)}')\n",
    "    y1 = np.zeros(len(X1))\n",
    "    y2 = np.ones(len(X2))\n",
    "    return np.append(X1, X2), np.append(y1, y2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books 8356\n",
      "code 1336\n"
     ]
    }
   ],
   "source": [
    "X, y = read_dataset('./books', './tasks')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чтобы кодировать строки текста, будем использовать простейший мешок слов. Реализуем сами (это моя реализация из соответсвующей домашки по ML)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class BoW:\n",
    "    def __init__(self, X: np.ndarray, voc_limit: int = 1000):\n",
    "        \"\"\"\n",
    "        Составляет словарь, который будет использоваться для векторизации предложений.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Массив строк (предложений) размерности (n_sentences, ),\n",
    "            по которому будет составляться словарь.\n",
    "        voc_limit : int\n",
    "            Максимальное число слов в словаре.\n",
    "\n",
    "        \"\"\"\n",
    "        self.vocab_size = None\n",
    "        self.d = {}\n",
    "        for sentence in X:\n",
    "            s = self.remaker(sentence)\n",
    "            for word in s:\n",
    "                if word in self.d:\n",
    "                    self.d[word] += 1\n",
    "                else:\n",
    "                    self.d[word] = 1\n",
    "        all_words = np.array(sorted(list(self.d.items()),key=lambda x: -x[1]))\n",
    "        self.d = all_words[:voc_limit, 0]     # убрать самые частые?\n",
    "        self.vocab_size = self.d.shape[0]\n",
    "        self.d = dict(np.concatenate((self.d[:, None], np.arange(len(self.d))[:, None]), axis=1))\n",
    "\n",
    "\n",
    "    def remaker(self, sentence):\n",
    "        simb = [',',\"'\", '!', '.', ',', '?', '*']\n",
    "        for x in simb:\n",
    "            sentence = sentence.replace(x, ' ')\n",
    "        return sentence.lower().split()\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Векторизует предложения.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Массив строк (предложений) размерности (n_sentences, ),\n",
    "            который необходимо векторизовать.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Матрица векторизованных предложений размерности (n_sentences, vocab_size)\n",
    "        \"\"\"\n",
    "        res = np.zeros((X.shape[0], self.vocab_size))\n",
    "        for i in range(X.shape[0]):\n",
    "            s = self.remaker(X[i])\n",
    "            for word in s:\n",
    "                if word in self.d:\n",
    "                    j = int(self.d[word])\n",
    "                    res[i, j] += 1\n",
    "        return res\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве модели воспользуемся наивным байесом\n",
    "Реализуем его сами"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, alpha: float):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float\n",
    "            Параметр аддитивной регуляризации.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.classes = None\n",
    "        self.f_probas = None\n",
    "        self.n_classes = None\n",
    "        self.y_probas = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Оценивает параметры распределения p(x|y) для каждого y.\n",
    "        \"\"\"\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        self.n_classes = len(self.classes)\n",
    "        n_dict = X.shape[1]\n",
    "        self.f_probas = np.zeros((n_dict, self.n_classes))\n",
    "        self.y_probas = counts / np.sum(counts)\n",
    "\n",
    "        for j in range(self.n_classes):\n",
    "            j_class = y== self.classes[j]\n",
    "            x_i_j_class = np.sum(X[j_class], axis=0)\n",
    "            all_j_class = np.sum(X[j_class])\n",
    "            self.f_probas[:, j] = (x_i_j_class + self.alpha) / (all_j_class + self.n_classes * self.alpha)\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ------\n",
    "        list\n",
    "            Предсказанный класс для каждого элемента из набора X.\n",
    "        \"\"\"\n",
    "        return np.array([self.classes[i] for i in np.argmax(self.log_proba(X), axis=1)])\n",
    "\n",
    "    def log_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Для каждого элемента набора X - логарифм вероятности отнести его к каждому классу.\n",
    "            Матрица размера (X.shape[0], n_classes)\n",
    "        \"\"\"\n",
    "        res = np.zeros((X.shape[0], self.n_classes))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self.n_classes):\n",
    "                res[i, j] = np.sum(X[i, :] * np.log(self.f_probas[:, j])) + self.y_probas[j]\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Научимся считать метрики"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_precision_recall_accuracy(y_pred, y_true):\n",
    "    true = sum(y_pred == y_true)\n",
    "    # все правильные делить на все неправильные - accuracy\n",
    "    all = y_pred.shape[0]\n",
    "    accuracy = true / all\n",
    "    precision, recall = np.array([]), np.array([])\n",
    "    classes = np.unique(list(y_pred) + list(y_true))\n",
    "    for item in classes:\n",
    "        TP = sum((y_pred == item) * (y_true == item))\n",
    "        FP = sum((y_pred == item) * (y_true != item))\n",
    "        FN = sum((y_pred != item) * (y_true == item))\n",
    "        if TP + FP > 0:\n",
    "            precision = np.append(precision, [TP / (TP + FP)])\n",
    "        else:\n",
    "            precision = np.append(precision, [0])\n",
    "        if TP + FN > 0:\n",
    "            recall = np.append(recall, [TP / (TP + FN)])\n",
    "        else:\n",
    "            recall = np.append(recall, [0])\n",
    "    precision = dict((c, precision[i]) for i, c in enumerate(classes))\n",
    "    recall = dict((c, recall[i]) for i, c in enumerate(classes))\n",
    "    print(f'precision: {precision} \\nrecall: {recall} \\naccuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучим модель и посмотрим, что получается"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "bow = BoW(X_train, voc_limit=1000)\n",
    "X_train_bow = bow.transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: {0.0: 0.9805309734513274, 1.0: 0.9877049180327869} \n",
      "recall: {0.0: 0.9981981981981982, 1.0: 0.8795620437956204} \n",
      "accuracy: 0.9814337287261475\n"
     ]
    }
   ],
   "source": [
    "predictor = NaiveBayes(0.001)\n",
    "predictor.fit(X_train_bow, y_train)\n",
    "get_precision_recall_accuracy(predictor.predict(X_test_bow), y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\"yield from range(number + 1, number + on_each_side + 1)\",\n",
    "              \"If the page range is larger than a given size, the whole range is not\"])\n",
    "tr = bow.transform(np.array(x))\n",
    "print(predictor.predict(tr))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно, что значения precision очень большие, значит, из того, что мы помечаем как нужный класс, мы очень часто угадываем\n",
    "Что касается recall, то поскольку у нас данные неравномерные, а так же из-за специфики данных, в словарь попадает очень много человеческих слов и мало кода. Поэтому мы очень точно находим человеческий текст и часто ошибаемся в случаях, когда сталкиваемся с кодом.\n",
    "На тех данных, на которых она обучилась, модель работает в общем не плохо."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}